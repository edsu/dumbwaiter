{
 "metadata": {
  "name": "",
  "signature": "sha256:7dcc5a71f796c96b032c845b80c7e3f6caaf4be3a05bd16eb1beaf25123a0b15"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# A Repeatable Extract-Tranform-Load Pipeline for NYPL Menus Data\n",
      "\n",
      "**Created:** 17 October 2014\n",
      "\n",
      "**Updated:** 28 October 2014, 19 November 2014, 3 December 2014, 14 December 2014, 8 January 2015\n",
      "\n",
      "**Authors:** Trevor Mu\u00f1oz and Katie Rawson\n",
      "\n",
      "&nbsp;"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Have you set environment variables?: \n",
      "\n",
      "* MENUS_LOG_HOME\n",
      "* MENUS_SOURCE_DATA\n",
      "* MENUS_OUTPUT_DIR\n",
      "* MENUS_ES_HOSTNAME, \n",
      "* MENUS_ES_HOST_PORT"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Acquiring Data (Extract)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the moment, the anti-bot protections on the New York Public Library's Web site make it impossible to grab the latest data download link directly. So, we download the data (as gzipped tar file) and package it with this script. \n",
      "\n",
      "The first stage of the pipeline unzips and untars the archive into four CSV files:\n",
      "\n",
      "* Dish.csv\n",
      "* Menu.csv\n",
      "* MenuItem.csv\n",
      "* MenuPage.csv\n",
      "\n",
      "&nbsp;"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import datetime\n",
      "import time\n",
      "import tarfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set up logging \u2026"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "import logging.handlers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LOG_FILENAME = os.path.join(os.environ['MENUS_LOG_HOME'], 'nypl_menus_data_transform.log')\n",
      "\n",
      "pipeline_logger = logging.getLogger('MenusDataTransformLogger')\n",
      "pipeline_logger.setLevel(logging.INFO)\n",
      "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
      "\n",
      "handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=500000, backupCount=10)\n",
      "handler.setFormatter(formatter)\n",
      "pipeline_logger.addHandler(handler)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_logger.info('Menus ETL Pipeline: Starting run \u2026')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "srcfile = [x for x in os.listdir(os.environ['MENUS_SOURCE_DATA']) if os.path.splitext(x)[1] == '.tgz'][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tar = tarfile.open(os.path.join(os.environ['MENUS_SOURCE_DATA'], srcfile))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_logger.info('Extracting source files \u2026')\n",
      "pipeline_logger.info('Listing contents of the tar package \u2026')\n",
      "for tf in tar.getmembers():\n",
      "    pipeline_logger.info('Name: {0} \\t Last Modified: {1}'.format(tf.name, time.ctime(tf.mtime)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_logger.info('Untarring and unzipping \u2026')\n",
      "tar.extractall(path=os.environ['MENUS_SOURCE_DATA'])\n",
      "\n",
      "for f in os.listdir(os.environ['MENUS_SOURCE_DATA']):\n",
      "    if f.endswith('csv'):\n",
      "        if os.path.isfile(os.path.join(os.environ['MENUS_SOURCE_DATA'], f)) == True:\n",
      "            pipeline_logger.info('{0} \u2026 \\u2713'.format(f))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tar.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Working with Data in DataFrames (Tranform)\n",
      "\n",
      "&nbsp;"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import json\n",
      "import pytz\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize_names(obj):\n",
      "    '''\n",
      "    Take a name as a string, converts the string\n",
      "    to lowercase, strips whitespace from beginning\n",
      "    and end, normalizes multiple internal whitespace\n",
      "    characters to a single space. E.g.:\n",
      "    \n",
      "    normalize_names('Chicken gumbo ') = 'chicken gumbo'\n",
      "    \n",
      "    '''\n",
      "    tokens = obj.strip().lower().split()\n",
      "    result = ' '.join(filter(None, tokens))\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fingerprint(obj):\n",
      "    \"\"\"\n",
      "    A modified version of the fingerprint clustering algorithm implemented by Open Refine.\n",
      "    See https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth\n",
      "    This does not normalize to ASCII characters since diacritics may be significant in this dataset\n",
      "    \"\"\"\n",
      "    alphanumeric_tokens = filter(None, re.split('\\W', obj))\n",
      "    seen = set()\n",
      "    seen_add = seen.add\n",
      "    deduped = sorted([i for i in alphanumeric_tokens if i not in seen and not seen_add(i)])\n",
      "    fingerprint = ' '.join(deduped)\n",
      "    \n",
      "    return fingerprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TOFIX: This code is very slow\n",
      "utc = pytz.utc\n",
      "\n",
      "def reformat_dates(obj):\n",
      "    naive_date_in = datetime.datetime.strptime(obj, '%Y-%m-%d %H:%M:%S %Z')\n",
      "    date_in = naive_date_in.replace(tzinfo=utc)\n",
      "    date_out = date_in.strftime('%Y%m%dT%H%M%S%z')\n",
      "    return date_out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reshape_data(obj):\n",
      "    '''\n",
      "    Takes JSON, loads it, and spits out a dictionary of slightly different shape\n",
      "    needed for bulk import by Elasticsearch.\n",
      "    \n",
      "    Input is JSON rather than simply another dictionary so I can punt on properly\n",
      "    serializing things like 'NaN' by leaving that to pandas own to_json() function.\n",
      "    '''\n",
      "    data = json.loads(obj)\n",
      "    action = {\n",
      "              \"_index\": \"menus\",\n",
      "              \"_type\": \"item\",\n",
      "              \"_id\": int(data['item_id']),\n",
      "              \"_source\": data\n",
      "              }\n",
      "    return action"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### DataFrame Loading"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import OrderedDict\n",
      "\n",
      "def load_dataframes(path):\n",
      "    \"\"\"\n",
      "    Takes a file path where CSV files to be processed are located\n",
      "    and returns a single merged pandas DataFrame with the relevant\n",
      "    data\n",
      "    \"\"\"\n",
      "    \n",
      "    pipeline_logger.info('Loading data from source files into memory \u2026')\n",
      "\n",
      "    LATEST_DISH_DATA_DF = pd.DataFrame.from_csv(os.path.join(path, 'Dish.csv'), index_col='id')\n",
      "    LATEST_ITEM_DATA_DF = pd.DataFrame.from_csv(os.path.join(path, 'MenuItem.csv'), index_col='dish_id')\n",
      "    LATEST_PAGE_DATA_DF = pd.DataFrame.from_csv(os.path.join(path, 'MenuPage.csv'), index_col='id')\n",
      "    LATEST_MENU_DATA_DF = pd.DataFrame.from_csv(os.path.join(path, 'Menu.csv'), index_col='id')\n",
      "\n",
      "    pipeline_logger.info('Data loaded. Starting transformations \u2026')\n",
      "    \n",
      "    # =================================\n",
      "    # \n",
      "    #  Dish.csv\n",
      "    #\n",
      "    # =================================\n",
      "    \n",
      "    NULL_APPEARANCES = LATEST_DISH_DATA_DF[LATEST_DISH_DATA_DF.times_appeared == 0]\n",
      "    pipeline_logger.info('Data set contains {0} dishes that appear 0 times \u2026'.format(len(NULL_APPEARANCES)))\n",
      "    \n",
      "    NON_NULL_DISH_DATA_DF = LATEST_DISH_DATA_DF[LATEST_DISH_DATA_DF.times_appeared != 0]\n",
      "    discarded_columns = [n for n in NON_NULL_DISH_DATA_DF.columns if n not in ['name', 'menus_appeared', 'times_appeared']]\n",
      "    pipeline_logger.info('Discarding columns from Dish.csv \u2026')\n",
      "    for discard in discarded_columns:\n",
      "        pipeline_logger.info('{0} \u2026 removed'.format(discard))\n",
      "        \n",
      "    TRIMMED_DISH_DATA_DF = NON_NULL_DISH_DATA_DF[['name', 'menus_appeared', 'times_appeared']]\n",
      "    pipeline_logger.info('Dish.csv contains {0} potentially-unique dish names before any normalization'.\n",
      "                     format(TRIMMED_DISH_DATA_DF.name.nunique()))\n",
      "    \n",
      "    TRIMMED_DISH_DATA_DF['normalized_name'] = TRIMMED_DISH_DATA_DF.name.map(normalize_names)\n",
      "    pipeline_logger.info(\n",
      "    'Dish.csv contains {0} potentially-unique dish names after normalizing whitespace and punctuation'\n",
      "    .format(TRIMMED_DISH_DATA_DF.normalized_name.nunique())\n",
      "    )\n",
      "    \n",
      "    TRIMMED_DISH_DATA_DF['fingerprint'] = TRIMMED_DISH_DATA_DF.normalized_name.map(fingerprint)\n",
      "    pipeline_logger.info(\n",
      "    'Dish.csv contains {0} unique fingerprint values'\n",
      "    .format(TRIMMED_DISH_DATA_DF.fingerprint.nunique())\n",
      "    )\n",
      "    #TRIMMED_DISH_DATA_DF.head()\n",
      "    \n",
      "    # =================================\n",
      "    # \n",
      "    # MenuItem.csv\n",
      "    #\n",
      "    # =================================\n",
      "    \n",
      "    pipeline_logger.info('Reformatting item dates \u2026')\n",
      "    LATEST_ITEM_DATA_DF['item_created_at'] = LATEST_ITEM_DATA_DF.created_at.map(reformat_dates)\n",
      "    LATEST_ITEM_DATA_DF['item_updated_at'] = LATEST_ITEM_DATA_DF.updated_at.map(reformat_dates)\n",
      "    pipeline_logger.info('Date reformatting complete \u2026')\n",
      "    \n",
      "    discarded_columns2 = [n for n in LATEST_ITEM_DATA_DF.columns if n not in \n",
      "                      ['id', 'menu_page_id', 'xpos', 'ypos', 'item_created_at', 'item_updated_at']]\n",
      "    pipeline_logger.info('Discarding columns from MenuItem.csv \u2026')\n",
      "    for discard2 in discarded_columns2:\n",
      "        pipeline_logger.info('{0} \u2026 removed'.format(discard2))\n",
      "        \n",
      "    TRIMMED_ITEM_DATA_DF = LATEST_ITEM_DATA_DF[['id', 'menu_page_id', 'xpos', 'ypos',\n",
      "                                           'item_created_at', 'item_updated_at']]\n",
      "    #TRIMMED_ITEM_DATA_DF.head()\n",
      "    \n",
      "    # =================================\n",
      "    # \n",
      "    # MenuPage.csv\n",
      "    #\n",
      "    # =================================\n",
      "    \n",
      "    #LATEST_PAGE_DATA_DF.head()\n",
      "    LATEST_PAGE_DATA_DF[['full_height', 'full_width']].astype(int, raise_on_error=False)\n",
      "    \n",
      "    # =================================\n",
      "    # \n",
      "    # Menu.csv\n",
      "    #\n",
      "    # =================================\n",
      "    \n",
      "    LATEST_MENU_DATA_DF.columns\n",
      "    \n",
      "    discarded_columns3 = [n for n in LATEST_MENU_DATA_DF.columns if n not in \n",
      "                      ['sponsor', 'location', 'date', 'page_count', 'dish_count']]\n",
      "    pipeline_logger.info('Discarding columns from Menu.csv \u2026')\n",
      "    for discard3 in discarded_columns3:\n",
      "        pipeline_logger.info('{0} \u2026 removed'.format(discard3))\n",
      "    \n",
      "    TRIMMED_MENU_DATA_DF = LATEST_MENU_DATA_DF[['sponsor', 'location', 'date',\n",
      "                                            'page_count', 'dish_count']]\n",
      "    #TRIMMED_MENU_DATA_DF.head()\n",
      "    \n",
      "    # =================================\n",
      "    # \n",
      "    # Merged DataFrames\n",
      "    #\n",
      "    # =================================\n",
      "    \n",
      "    pipeline_logger.info('Merging dataframes \u2026')\n",
      "    MERGED_ITEM_PAGES_DF = pd.merge(TRIMMED_ITEM_DATA_DF, LATEST_PAGE_DATA_DF, \n",
      "                                left_on='menu_page_id', right_index=True, )\n",
      "    \n",
      "    MERGED_ITEM_PAGES_DF.columns = ['item_id', 'menu_page_id', 'xpos', 'ypos', \n",
      "                                'item_created_at', 'item_updated_at', 'menu_id', 'page_number', \n",
      "                                'image_id', 'full_height', 'full_width', 'uuid']\n",
      "    #MERGED_ITEM_PAGES_DF.head()\n",
      "    \n",
      "    MERGED_ITEM_PAGES_MENUS_DF = pd.merge(TRIMMED_MENU_DATA_DF, MERGED_ITEM_PAGES_DF, \n",
      "                                      left_index=True, right_on='menu_id')\n",
      "    \n",
      "    FULL_MERGE = pd.merge(MERGED_ITEM_PAGES_MENUS_DF, TRIMMED_DISH_DATA_DF, \n",
      "                      left_index=True, right_index=True)\n",
      "    #FULL_MERGE.head()\n",
      "    \n",
      "    FOR_JSON_OUTPUT = FULL_MERGE.reset_index()\n",
      "    \n",
      "    FOR_JSON_OUTPUT.columns\n",
      "    renamed_columns = ['dish_id', 'menu_sponsor', 'menu_location', 'menu_date', 'menu_page_count', \n",
      "                   'menu_dish_count', 'item_id', 'menu_page_id', 'item_xpos', 'item_ypos', \n",
      "                   'item_created_at', 'item_updated_at', 'menu_id', 'menu_page_number', 'image_id', \n",
      "                   'page_image_full_height', 'page_image_full_width', 'page_image_uuid', 'dish_name', \n",
      "                   'dish_menus_appeared', 'dish_times_appeared', 'dish_normalized_name', 'dish_name_fingerprint']\n",
      "    FOR_JSON_OUTPUT.columns = renamed_columns\n",
      "    \n",
      "    FOR_JSON_OUTPUT[['menu_page_number', 'dish_id', 'item_id', 'menu_page_id', 'menu_id']].astype(int, raise_on_error=False)\n",
      "    \n",
      "    FOR_JSON_OUTPUT['dish_uri']= FOR_JSON_OUTPUT.dish_id.map(lambda x: 'http://menus.nypl.org/dishes/{0}'.format(int(x)))\n",
      "    FOR_JSON_OUTPUT['item_uri']= FOR_JSON_OUTPUT.item_id.map(lambda x: 'http://menus.nypl.org/menu_items/{0}/edit'\n",
      "                                               .format(int(x)))\n",
      "    FOR_JSON_OUTPUT['menu_page_uri'] = FOR_JSON_OUTPUT.menu_page_id.map(lambda x: 'http://menus.nypl.org/menu_pages/{0}'\n",
      "                                                          .format(int(x)))\n",
      "    FOR_JSON_OUTPUT['menu_uri'] = FOR_JSON_OUTPUT.menu_id.map(lambda x:'http://menus.nypl.org/menus/{0}'\n",
      "                                                .format(int(x)))\n",
      "    \n",
      "    FOR_JSON_OUTPUT.fillna('null')\n",
      "    \n",
      "    pipeline_logger.info('Merged dataframe ready')\n",
      "    #FOR_JSON_OUTPUT.head()\n",
      "    \n",
      "    # df.iterrows is a generator that yields a positional index and a Series,\n",
      "    # call the to_json method on the series\n",
      "    return (reshape_data(row.to_json()) for i, row in FOR_JSON_OUTPUT.iterrows())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load (to Elasticsearch)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import elasticsearch"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.environ['MENUS_ES_HOSTNAME'] = 'localhost'\n",
      "os.environ['MENUS_ES_HOST_PORT'] = '5000'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_logger.info('Verifying Elasticsearch server is ready to receive data \u2026')\n",
      "es = elasticsearch.Elasticsearch([{'host': os.environ['MENUS_ES_HOSTNAME'], 'port': os.environ['MENUS_ES_HOST_PORT']}])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "INDEX_NAME = 'menus'\n",
      "TYPE_NAME = 'item'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check if the index already exists and, if so, delete it to keep things idempotent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if es.indices.exists(INDEX_NAME):\n",
      "    pipeline_logger.info(\"deleting '{0}' index...\".format(INDEX_NAME))\n",
      "    res = es.indices.delete(index = INDEX_NAME)\n",
      "    pipeline_logger.info(\" response: '{0}'\".format(res))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_logger.info(\"creating '{0}' index...\".format(INDEX_NAME))\n",
      "res = es.indices.create(index = INDEX_NAME)\n",
      "pipeline_logger.info(\" response: '{0}'\".format(res))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "item_mapping = {\"properties\": {\n",
      "    'item_updated_at': {\n",
      "        \"type\": \"date\",\n",
      "        \"format\": \"basic_date_time_no_millis\"\n",
      "    },\n",
      "    'menu_page_id': {\n",
      "        \"type\": \"long\",\n",
      "    } ,\n",
      " 'menu_sponsor': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    },\n",
      " 'menu_page_count': {\n",
      "    \"type\": \"double\"\n",
      "    },\n",
      " 'item_ypos': {\n",
      "    \"type\": \"double\"\n",
      "    },\n",
      " 'dish_normalized_name': {\n",
      "    \"type\": \"string\"\n",
      "    },\n",
      " 'menu_id': {\n",
      "        \"type\": \"long\",\n",
      "    },\n",
      " 'dish_times_appeared': {\n",
      "    \"type\": \"double\"\n",
      "    },\n",
      " 'menu_location': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    },\n",
      " 'dish_menus_appeared': {\n",
      "    \"type\": \"double\"\n",
      "    },\n",
      " 'menu_uri': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    },\n",
      " 'menu_page_uri': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    },\n",
      " 'page_image_uuid': {\n",
      "    \"type\": \"string\"\n",
      "    },\n",
      " 'menu_page_number': {\n",
      "    \"type\": \"double\"\n",
      "    },\n",
      " 'item_created_at': {\n",
      "    \"type\": \"date\",\n",
      "    \"format\": \"basic_date_time_no_millis\"\n",
      "    },\n",
      " 'dish_id': {\n",
      "    \"type\": \"long\",\n",
      "    },\n",
      " 'dish_name': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    },\n",
      " 'item_uri': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    },\n",
      " 'item_id': {\n",
      "    \"type\": \"long\",\n",
      "    },\n",
      " 'image_id': {\n",
      "    \"type\": \"long\",\n",
      "    },\n",
      " 'item_xpos': {\n",
      "    \"type\": \"double\"\n",
      "    },\n",
      " 'dish_name_fingerprint': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    },\n",
      " 'dish_uri': {\n",
      "    \"type\": \"string\",\n",
      "    \"index\": \"not_analyzed\"\n",
      "    }\n",
      "}\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_logger.info(\"Creating mapping for '{0}' index...\".format(INDEX_NAME))\n",
      "res = es.indices.put_mapping(index='menus', doc_type='item', body=item_mapping)\n",
      "pipeline_logger.info(\" response: '{0}'\".format(res))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def chunk_actions(l, size):\n",
      "#     for i in range(0, len(l), size):\n",
      "#         yield l[i:i+size]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from elasticsearch import helpers\n",
      "\n",
      "# pipeline_logger.info('Loading data into Elasticsearch \u2026')\n",
      "\n",
      "# for batch in chunk_actions(DOCS_TO_INDEX, 100000):\n",
      "#     pipeline_logger.info(es.indices.stats(INDEX_NAME))\n",
      "#     helpers.bulk(es, batch)\n",
      "\n",
      "# pipeline_logger.info('Elasticsearch index ready!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Reshaping DataFrame JSON to ES bulk upload JSON"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # From this SO answer: http://stackoverflow.com/questions/24527006/split-a-generator-into-chunks-without-pre-walking-it\n",
      "# from itertools import chain, islice\n",
      "\n",
      "# def chunks(iterable, size=10):\n",
      "#     iterator = iter(iterable)\n",
      "#     for first in iterator:\n",
      "#         yield chain([first], islice(iterator, size - 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from elasticsearch import helpers\n",
      "\n",
      "es_action_gen = load_dataframes(os.environ['MENUS_SOURCE_DATA'])\n",
      "\n",
      "pipeline_logger.info('Preparing data to load into Elasticsearch \u2026')\n",
      "try:\n",
      "    for ok, result in helpers.streaming_bulk(es, es_action_gen, chunk_size=1000):\n",
      "        action, result = result.popitem()\n",
      "        doc_id = '/menus/item/{0}'.format(result['_id'])\n",
      "        if not ok:\n",
      "            pipeline_logger.error('Failed to {0} document {1}: {2}'.format(action, doc_id, result['error']))\n",
      "        else:\n",
      "            pipeline_logger.info(doc_id + 'succeeded')\n",
      "except BaseException as e:\n",
      "    pipeline_logger.error('Something went wrong: {0}'.format(str(e)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from elasticsearch import helpers\n",
      "\n",
      "# ROWGEN = load_dataframes(os.environ['MENUS_SOURCE_DATA'])\n",
      "\n",
      "# pipeline_logger.info('Preparing data to load into Elasticsearch \u2026')\n",
      "# for batch in chunks(ROWGEN, size=100000):\n",
      "#     actions = [reshape_json(doc) for doc in batch]\n",
      "#     pipeline_logger.info('{0} actions ready for loading \u2026'.format(len(actions)))\n",
      "#     #pipeline_logger.info(actions[0])\n",
      "#     res = helpers.bulk(es, actions)\n",
      "#     pipeline_logger.info(res)\n",
      "#     #pipeline_logger.info(es.indices.stats(INDEX_NAME))\n",
      "    \n",
      "# #pipeline_logger.info('Generator exhausted')\n",
      "# pipeline_logger.info('Elasticsearch index ready!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}